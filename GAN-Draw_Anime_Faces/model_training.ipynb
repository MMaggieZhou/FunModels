{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/FunModels/blob/main/GAN-Draw_Anime_Faces/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK_I7KqkoZVj"
      },
      "source": [
        "# Draw Anime Faces With Generative Adversarial Network\n",
        "\n",
        "- The model uses DCGAN architecture per https://arxiv.org/abs/1511.06434\n",
        "- Tensorflow is used as the training framework \n",
        "- The code isn't very super robust as validations are left to be implemented "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I08L7GiojrP"
      },
      "source": [
        "## SET UP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google Drive"
      ],
      "metadata": {
        "id": "HKftpHfA0yDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "S14DtgS1o1oX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75874bc-99d4-4050-9ee5-bb3c9452693e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "CHECKPOINT_DIR = '/content/gdrive/MyDrive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxbrMAVxovUU"
      },
      "source": [
        "### Download the dataset \n",
        "The dataset is collected by https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdEVoLFoYIH",
        "outputId": "bd454698-a4eb-4113-c8c9-4fb73118e9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p\n",
            "To: /content/crypko_data.zip\n",
            "100% 452M/452M [00:01<00:00, 231MB/s]\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = '.' \n",
        "# a pypi package to download large file from google drive \n",
        "!gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p -O \"{DATA_DIR}/crypko_data.zip\"\n",
        "!unzip -q -o \"{DATA_DIR}/crypko_data.zip\" -d \"{DATA_DIR}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfwsQHztcPP"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6VBt4P8YLQnZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auocQWduLJc"
      },
      "source": [
        "## Data Preprocessing \n",
        "1. Load Dataset From Directory\n",
        "2. Resize the imaqe\n",
        "3. **Normalize Image: it's very very very important that the image data is withint [-1, 1] for neural network!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIam_D6Mz48I"
      },
      "outputs": [],
      "source": [
        "def load_dataset(directory_path, batch_size, image_size): \n",
        "    images = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory_path, \n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        image_size=image_size\n",
        "    )\n",
        "    normalization_layer = tf.keras.layers.Rescaling(2.0/255, offset=-1)\n",
        "    return images.map(lambda x: normalization_layer(x))\n",
        "\n",
        "def data_loading_test(): \n",
        "    image_batches = load_dataset(DATA_DIR, 64, (64, 64))\n",
        "    # TODO: \n",
        "    # 1.validate dimension is (batch_size, height, width, 3)\n",
        "    # 2.validate that all values are within [-1, 1]\n",
        "    # 3.display 16 images \n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    data = image_batches[0].take(16).map(lambda image: tf.keras.layers.Rescaling(255/2.0, offset=127.5)(image))\n",
        "    # TODO: better way of display tensors \n",
        "    for i, image in enumerate(data):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nthVTZ7zHfgx"
      },
      "source": [
        "## Define Model\n",
        "Two Models are defined with Keras layers, aka Generator model and Discriminator model. \n",
        "\n",
        "DCGAN key points: \n",
        "- Generator consists of convolutional -transpose layers that given a latent vector of smaller dimension, generates a 2D image with larger dimension\n",
        "- discriminator consists of convolutional layers, takes the large dimension 2D image, convolutes and eventually generate a binary output\n",
        "- apply batch normalization after each layer, except for output layer for generator and input layer for discriminator. \n",
        "- apply random normal distribution for weight initialization convolution(transpose) layers \n",
        "- apply ReLU activation for convolution transpose layers and leaky ReLU for convolution layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Components for DCGAN tricks"
      ],
      "metadata": {
        "id": "g96qESod1sW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Clip(tf.keras.constraints.Constraint):\n",
        "    def __init__(self, clip_val, enable):\n",
        "        self.clip_val = clip_val\n",
        "        self.enable = enable\n",
        "\n",
        "    def __call__(self, w):\n",
        "        if self.enable:\n",
        "            return tf.math.minimum(self.clip_val, tf.math.maximum(w, -self.clip_val))\n",
        "        return w\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"clip_val\": self.clip_val,\n",
        "            \"enable\": self.enable\n",
        "        }\n",
        "\n",
        "def clip_test():\n",
        "    weight = tf.constant((-1.0, 1.0))\n",
        "    Clip(clip_val=0.1, enable=False)(weight)\n",
        "    Clip(clip_val=0.1, enable=True)(weight)\n",
        "\n",
        "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02)\n",
        "\n",
        "def add_dense_layer_for_noise(\n",
        "    model,\n",
        "    input_dim, \n",
        "    output_dim,\n",
        "): \n",
        "    model.add(layers.Dense(\n",
        "        units=output_dim, \n",
        "        input_shape=(input_dim,), \n",
        "        use_bias=False\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization(\n",
        "        gamma_initializer=gamma_initializer\n",
        "    ))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "# image size will be doubled \n",
        "def add_conv2d_transpose(\n",
        "    model,\n",
        "    num_output_filters, \n",
        "    add_batch_norm=True\n",
        "):\n",
        "    model.add(layers.Conv2DTranspose(\n",
        "        num_output_filters, \n",
        "        5, # filter size\n",
        "        strides=2, \n",
        "        padding='same', \n",
        "        use_bias=False, \n",
        "        kernel_initializer=w_init,\n",
        "    ))\n",
        "    if add_batch_norm:\n",
        "      model.add(layers.BatchNormalization(\n",
        "          gamma_initializer=gamma_initializer\n",
        "      ))\n",
        "      model.add(layers.ReLU())\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d_for_input(model, input_dim, num_output_filters, weight_cilp):\n",
        "    model.add(layers.Conv2D(\n",
        "      num_output_filters, \n",
        "      5, # filter size\n",
        "      strides=2, \n",
        "      padding='same',\n",
        "      input_shape=[input_dim, input_dim, 3],\n",
        "      kernel_initializer=w_init,\n",
        "      kernel_constraint=weight_cilp,\n",
        "    ))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d(\n",
        "    model, num_output_filters, weight_clip, filter_size=5, \n",
        "    use_batch_norm=True, padding='same', stride=2\n",
        "):\n",
        "    model.add(layers.Conv2D(\n",
        "        num_output_filters, \n",
        "        filter_size, # filter size\n",
        "        strides=stride, \n",
        "        padding=padding,\n",
        "        kernel_initializer=w_init,\n",
        "        kernel_constraint=weight_clip,\n",
        "    ))\n",
        "    if use_batch_norm:\n",
        "      model.add(layers.BatchNormalization(\n",
        "          gamma_initializer=gamma_initializer, \n",
        "          beta_constraint=weight_clip, \n",
        "          gamma_constraint=weight_clip\n",
        "      ))\n",
        "      model.add(layers.LeakyReLU(0.2))"
      ],
      "metadata": {
        "id": "mmMLZf0XWyFc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {
        "id": "Af1v6zWu13Fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "po5f1FHj7Zl1"
      },
      "outputs": [],
      "source": [
        "def create_unconditional_generator(\n",
        "    noise_dim,\n",
        "    image_dim, # output image\n",
        "):\n",
        "    model = tf.keras.Sequential()\n",
        "    add_dense_layer_for_noise(\n",
        "        model, input_dim=noise_dim, \n",
        "        output_dim=(image_dim * 8) * (image_dim/16) * (image_dim/16)\n",
        "    )\n",
        "\n",
        "    model.add(layers.Reshape(\n",
        "        (int(image_dim/16), int(image_dim/16), image_dim * 8))\n",
        "    ) # image_dim/16 * image_dim/16 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, image_dim * 4) # image_dim/8 * image_dim/8 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 2) # image_dim/4 * image_dim/4 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 1) # image_dim/2 * image_dim/2 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, 3, add_batch_norm=False) # image_dim * image_dim * 3\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "def create_discriminator(image_dim, enable_clip, clip_value): \n",
        "    model = tf.keras.Sequential()\n",
        "    clip = Clip(clip_val=clip_value, enable=enable_clip)\n",
        "    add_conv2d_for_input(model, image_dim, image_dim, clip) # (image_dim /2, image_dim /2, image_dim)\n",
        "\n",
        "    add_conv2d(model, image_dim * 2, clip) # (image_dim /4, image_dim /4, image_dim * 2)\n",
        "    add_conv2d(model, image_dim * 4, clip) # (image_dim /8, image_dim /8, image_dim * 4)\n",
        "    add_conv2d(model, image_dim * 8, clip) # (image_dim /16, image_dim /16, image_dim * 8)\n",
        "\n",
        "    add_conv2d(model, 1, clip, filter_size=int(image_dim/16), use_batch_norm=False, padding='valid', stride=1) # (1, 1, 1)\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MofdVvWU-RxF"
      },
      "outputs": [],
      "source": [
        "def validate_generator():\n",
        "  # TODO: validate layers dimensions\n",
        "  generator = create_unconditional_generator(100, 64)\n",
        "\n",
        "\n",
        "def validate_discriminator():\n",
        "  # TODO\n",
        "  discriminator = create_discriminator(64)\n",
        "\n",
        "def test_output_values():\n",
        "    generator = create_unconditional_generator(100, 64)\n",
        "    discriminator = create_discriminator(64)\n",
        "    noise = tf.random.normal([10, 100])\n",
        "    fake_images = generator(noise, training=True)\n",
        "    output = discriminator(fake_images, training=True)\n",
        "    print(tf.reduce_mean(output))\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hazX8bT1-sEE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "jkQ7czF722lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions \n",
        "def discriminator_loss_wasserstein(real_output, fake_output): \n",
        "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "def generator_loss_wasserstein(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "def discriminator_loss_entropy(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss_entropy(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "MfFdV1yZd4nL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Training Step"
      ],
      "metadata": {
        "id": "VFnih7f53Blb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xrKWpvuoX9Fr"
      },
      "outputs": [],
      "source": [
        "# training step \n",
        "\n",
        "# may use @tf.function for optimization, but have to deal with dynamic variable step\n",
        "def train_step(\n",
        "    image_batch, batch_size, noise_dim, generator, discriminator, \n",
        "    generator_optimizer, discriminator_optimizer, discriminator_loss_func, \n",
        "    generator_loss_func, step\n",
        "):\n",
        "    # TODO: validate that image_batch size is same as batch_size\n",
        "    # TODOï¼šfine tune ratio of frequency that generator and discriminator are trained\n",
        "    metrics = {}\n",
        "    # 1. update discriminator \n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(image_batch, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        disc_loss = discriminator_loss_func(real_output, fake_output)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    metrics['D_loss'] = disc_loss\n",
        "  \n",
        "    # 2.update generator\n",
        "    if (step + 1) % 5 == 0:\n",
        "        noise = tf.random.normal([batch_size, noise_dim])\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            generated_images = generator(noise, training=True)\n",
        "            fake_output = discriminator(generated_images, training=True)\n",
        "            gen_loss = generator_loss_func(fake_output)\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "        metrics['G_loss'] = gen_loss\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Progress Tracking"
      ],
      "metadata": {
        "id": "Vt2f96aDb7Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Metrics():\n",
        "    def __init__(self, epoch, loss_d, loss_g):\n",
        "        self.x = [i for i in range(1, epoch + 1)]\n",
        "        self.loss_d = loss_d.tolist()\n",
        "        self.loss_g = loss_g.tolist()\n",
        "\n",
        "    def on_epoch_end(self, epoch, metrics):\n",
        "        self.x.append(epoch)\n",
        "        self.loss_d.append(float(metrics['D_loss']))\n",
        "        self.loss_g.append(float(metrics['G_loss']))\n",
        "\n",
        "        #clear_output(wait=True)\n",
        "        if epoch % 5 == 0:\n",
        "            f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "            ax1.plot(self.x, self.loss_d, label=\"D_loss\")\n",
        "            ax1.legend()\n",
        "            ax2.plot(self.x, self.loss_g, label=\"G_loss\")\n",
        "            ax2.legend()\n",
        "            plt.show()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return {\n",
        "            'D_loss': self.loss_d,\n",
        "            'G_loss': self.loss_g\n",
        "        }\n",
        "\n",
        "class ProgressManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        output_path, \n",
        "        generator, \n",
        "        discriminator, \n",
        "        generator_optimizer, \n",
        "        discriminator_optimizer, \n",
        "        num_batches, \n",
        "        persist_enabled\n",
        "    ):\n",
        "        self.generator = generator\n",
        "        self.num_batches = num_batches\n",
        "        self.persist_enabled = persist_enabled\n",
        "\n",
        "        self.output_path = output_path\n",
        "        self.samples_path = f'{output_path}/samples'\n",
        "        if self.persist_enabled and not os.path.exists(self.samples_path):\n",
        "            os.makedirs(self.samples_path)\n",
        "        self.checkpoint_path = f'{output_path}/checkpoints'\n",
        "\n",
        "        self.ckpt = tf.train.Checkpoint(\n",
        "            epoch=tf.Variable(0),\n",
        "            generator=generator,\n",
        "            discriminator=discriminator,\n",
        "            generator_optimizer=generator_optimizer,\n",
        "            discriminator_optimizer=discriminator_optimizer,\n",
        "            generator_loss=tf.Variable([], shape=tf.TensorShape(None)),\n",
        "            discriminator_loss=tf.Variable([], shape=tf.TensorShape(None)),\n",
        "        )\n",
        "        \n",
        "        self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.checkpoint_path, max_to_keep=20)\n",
        "        if self.ckpt_manager.latest_checkpoint:\n",
        "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
        "            print(\"Restored from {}\".format(self.ckpt_manager.latest_checkpoint))\n",
        "\n",
        "        self.last_epoch = int(self.ckpt.epoch)\n",
        "        self.metrics = Metrics(\n",
        "            self.last_epoch, \n",
        "            self.ckpt.discriminator_loss.numpy(), \n",
        "            self.ckpt.generator_loss.numpy()\n",
        "        )\n",
        "\n",
        "    def on_new_epoch(self, epoch):\n",
        "        print(\"Epoch: \", epoch)\n",
        "        self.pbar = tf.keras.utils.Progbar(target=self.num_batches, stateful_metrics=[])\n",
        "\n",
        "    def on_step_end(self, batch_number, metrics):\n",
        "        self.pbar.update(batch_number, values=metrics.items(), finalize=False)\n",
        "        self.last_metrics = metrics\n",
        "\n",
        "    def on_epoch_end(self, test_noises, test_image_grid_size):\n",
        "        self.ckpt.epoch.assign_add(1) # epoch should be bumped right after the training of an epoch is finished. as it indicate the epoch reflecting its current values \n",
        "        self.last_epoch += 1\n",
        "        metrics_history = self.metrics.get_metrics()\n",
        "        self.ckpt.discriminator_loss.assign(metrics_history['D_loss'])\n",
        "        self.ckpt.generator_loss.assign(metrics_history['G_loss'])\n",
        "        \n",
        "        self.pbar.update(self.num_batches, values=self.last_metrics.items(), finalize=True)\n",
        "        self.metrics.on_epoch_end(self.last_epoch, self.last_metrics)\n",
        "\n",
        "        self._save_plot(self.generator(test_noises, training=False), test_image_grid_size)\n",
        "        if self.persist_enabled and self.last_epoch % 10 == 0:\n",
        "            ckpt_save_path = self.ckpt_manager.save()\n",
        "            print('Saving checkpoint for epoch {} at {}'.format(self.last_epoch, ckpt_save_path))\n",
        "\n",
        "    def _save_plot(self, examples, n):\n",
        "        examples = (examples + 1) / 2.0 # (0-1 float or 0-255 int) for RGB values\n",
        "        for i in range(n * n):\n",
        "            plt.subplot(n, n, i+1)\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(examples[i])  \n",
        "        filename = f\"{self.samples_path}/generated_plot_epoch-{self.last_epoch}.png\"\n",
        "        if self.persist_enabled:\n",
        "            plt.savefig(filename)\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "TgeUadxvb-Ap"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resumable Training Process"
      ],
      "metadata": {
        "id": "nVEp9blW3f2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MGdoTgKvDVHu"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "IMAGE_DIM = 64\n",
        "NOISE_DIM = 100\n",
        "TEST_IMAGE_GRID_SIZE = 4\n",
        "\n",
        "CLIP_VALUE_FOR_WGAN = 0.03\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCH = 50\n",
        "\n",
        "\n",
        "def initiate_objects(use_wgan, training_name, save_model, num_batches):\n",
        "    if use_wgan:\n",
        "        generator_loss_func = generator_loss_wasserstein\n",
        "        discriminator_loss_func = discriminator_loss_wasserstein\n",
        "        generator_optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "        discriminator_optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "        enable_weight_clip = True\n",
        "    else:\n",
        "        generator_loss_func = generator_loss_entropy\n",
        "        discriminator_loss_func = discriminator_loss_entropy\n",
        "        generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
        "        discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
        "        enable_weight_clip = False\n",
        "\n",
        "    generator = create_unconditional_generator(NOISE_DIM, IMAGE_DIM)\n",
        "    generator.summary()\n",
        "    discriminator = create_discriminator(IMAGE_DIM, enable_weight_clip, CLIP_VALUE_FOR_WGAN)\n",
        "    discriminator.summary()\n",
        "\n",
        "    model = 'WGAN' if use_wgan else 'DCGAN'\n",
        "    progress_manager = ProgressManager(\n",
        "        f'{CHECKPOINT_DIR}/{model}/{training_name}', \n",
        "        generator, \n",
        "        discriminator, \n",
        "        generator_optimizer, \n",
        "        discriminator_optimizer,\n",
        "        num_batches,\n",
        "        save_model,\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        generator, discriminator, generator_loss_func, discriminator_loss_func, \n",
        "        generator_optimizer, discriminator_optimizer, progress_manager\n",
        "    ) \n",
        "\n",
        "def train(training_name, save_model=True, speed_up_epoch=False, use_wgan=True):\n",
        "    # create/load data\n",
        "    tf.random.set_seed(2022) # so that we can get the same test noises each time after resuming from checkpoint  \n",
        "    test_noises = tf.random.normal([TEST_IMAGE_GRID_SIZE ** 2, NOISE_DIM])\n",
        "    image_batches = load_dataset(f'{DATA_DIR}/faces', BATCH_SIZE, (IMAGE_DIM, IMAGE_DIM))\n",
        "\n",
        "    # initiate objects for training: models, optimizers, progress manager \n",
        "    (\n",
        "        generator, \n",
        "        discriminator, \n",
        "        generator_loss_func, \n",
        "        discriminator_loss_func, \n",
        "        generator_optimizer, \n",
        "        discriminator_optimizer,\n",
        "        progress_manager,\n",
        "    ) = initiate_objects(use_wgan, training_name, save_model, int(image_batches.cardinality()))\n",
        "\n",
        "    # actual training cycles\n",
        "    step = 0\n",
        "    for epoch in range(progress_manager.last_epoch + 1, progress_manager.last_epoch + NUM_EPOCH + 1):\n",
        "        progress_manager.on_new_epoch(epoch)\n",
        "        i = 0 \n",
        "        for image_batch in image_batches:\n",
        "            if speed_up_epoch and i == 10:\n",
        "                break\n",
        "            progress_manager.on_step_end(i, train_step(\n",
        "                image_batch=image_batch, \n",
        "                batch_size=BATCH_SIZE, \n",
        "                noise_dim=NOISE_DIM, \n",
        "                generator=generator, \n",
        "                discriminator=discriminator, \n",
        "                generator_optimizer=generator_optimizer, \n",
        "                discriminator_optimizer=discriminator_optimizer, \n",
        "                discriminator_loss_func=discriminator_loss_func, \n",
        "                generator_loss_func=generator_loss_func,\n",
        "                step=step,\n",
        "            ))\n",
        "            i += 1\n",
        "            step += 1\n",
        "        progress_manager.on_epoch_end(test_noises, TEST_IMAGE_GRID_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Process"
      ],
      "metadata": {
        "id": "8ZsqQLk53nHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train(training_name='11_05_2022_test1', save_model=True, speed_up_epoch=True,)\n",
        "#train(training_name='11_05_2022_test2', save_model=True, speed_up_epoch=True,)\n",
        "#train(training_name='11_05_2022', save_model=True, speed_up_epoch=False,)\n",
        "#train(training_name='11_07_2022_test1', save_model=True, speed_up_epoch=True, use_wgan=False)\n",
        "train(training_name='11_07_2022', save_model=True, speed_up_epoch=False, use_wgan=False)"
      ],
      "metadata": {
        "id": "JdVxDF8sOPuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70d7e2c-7f67-45d5-bb84-c7f9d6b5edc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 71314 files belonging to 1 classes.\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 8192)              819200    \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 8192)             32768     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_32 (ReLU)             (None, 8192)              0         \n",
            "                                                                 \n",
            " reshape_8 (Reshape)         (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_32 (Conv2D  (None, 8, 8, 256)        3276800   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 8, 8, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_33 (ReLU)             (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_33 (Conv2D  (None, 16, 16, 128)      819200    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_34 (ReLU)             (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_34 (Conv2D  (None, 32, 32, 64)       204800    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_35 (ReLU)             (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_35 (Conv2D  (None, 64, 64, 3)        4800      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,159,360\n",
            "Trainable params: 5,142,080\n",
            "Non-trainable params: 17,280\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 32, 32, 64)        4864      \n",
            "                                                                 \n",
            " leaky_re_lu_31 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_32 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 8, 8, 256)         819456    \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 8, 8, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_33 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 4, 4, 512)         3277312   \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 4, 4, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_34 (LeakyReLU)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 1, 1, 1)           8193      \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,318,337\n",
            "Trainable params: 4,316,545\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Epoch:  1\n",
            "1072/1115 [===========================>..] - ETA: 7s - D_loss: 0.4689 - G_loss: 3.1267"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var=tf.Variable([],shape=tf.TensorShape(None))\n",
        "var.assign([1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-A8KFQJ0awx",
        "outputId": "c7e4d8b4-48ac-42d1-c514-04fd01069fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=<unknown> dtype=float32, numpy=array([1., 2.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMjf9Np1bGkpDr5Rd0B282L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}