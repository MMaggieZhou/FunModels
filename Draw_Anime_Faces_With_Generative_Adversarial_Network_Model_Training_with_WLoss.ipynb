{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/FunModels/blob/main/Draw_Anime_Faces_With_Generative_Adversarial_Network_Model_Training_with_WLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK_I7KqkoZVj"
      },
      "source": [
        "# Draw Anime Faces With Generative Adversarial Network\n",
        "\n",
        "- The model uses DCGAN architecture per https://arxiv.org/abs/1511.06434\n",
        "- Tensorflow is used as the training framework \n",
        "- The code isn't very super robust as validations are left to be implemented "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I08L7GiojrP"
      },
      "source": [
        "## Set up the environment \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S14DtgS1o1oX"
      },
      "outputs": [],
      "source": [
        "WORKSPACE_DIR = '.' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxbrMAVxovUU"
      },
      "source": [
        "### Download the dataset \n",
        "The dataset is collected by https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdEVoLFoYIH",
        "outputId": "2b1277a8-9912-4f02-ce4b-a5f30e9396cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p\n",
            "To: /content/crypko_data.zip\n",
            "100% 452M/452M [00:02<00:00, 202MB/s]\n"
          ]
        }
      ],
      "source": [
        "# a pypi package to download large file from google drive \n",
        "!gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p -O \"{WORKSPACE_DIR}/crypko_data.zip\"\n",
        "!unzip -q -o \"{WORKSPACE_DIR}/crypko_data.zip\" -d \"{WORKSPACE_DIR}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfwsQHztcPP"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6VBt4P8YLQnZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auocQWduLJc"
      },
      "source": [
        "## Data Preprocessing \n",
        "1. Load Dataset From Directory\n",
        "2. Resize the imaqe\n",
        "3. **Normalize Image: it's very very very important that the image data is withint [-1, 1] for neural network!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIam_D6Mz48I"
      },
      "outputs": [],
      "source": [
        "def load_dataset(directory_path, batch_size, image_size): \n",
        "    images = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory_path, \n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        image_size=image_size\n",
        "    )\n",
        "    normalization_layer = tf.keras.layers.Rescaling(2.0/255, offset=-1)\n",
        "    return images.map(lambda x: normalization_layer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nCCNafSI4s5k"
      },
      "outputs": [],
      "source": [
        "def validate_data_loading(): \n",
        "  image_batches = load_dataset(WORKSPACE_DIR, 64, (64, 64))\n",
        "  # TODO: \n",
        "  # 1.validate dimension is (batch_size, height, width, 3)\n",
        "  # 2.validate that all values are within [-1, 1]\n",
        "  # 3.display 16 images \n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  data = image_batches[0].take(16).map(lambda image: tf.keras.layers.Rescaling(255/2.0, offset=127.5)(image))\n",
        "  # TODO: better way of display tensors \n",
        "  for i, image in enumerate(data):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nthVTZ7zHfgx"
      },
      "source": [
        "## Define Model Architecture \n",
        "Two Models are defined with Keras layers, aka Generator model and Discriminator model. \n",
        "\n",
        "DCGAN key points: \n",
        "- Generator consists of convolutional -transpose layers that given a latent vector of smaller dimension, generates a 2D image with larger dimension\n",
        "- discriminator consists of convolutional layers, takes the large dimension 2D image, convolutes and eventually generate a binary output\n",
        "- apply batch normalization after each layer, except for output layer for generator and input layer for discriminator. \n",
        "- apply random normal distribution for weight initialization convolution(transpose) layers \n",
        "- apply ReLU activation for convolution transpose layers and leaky ReLU for convolution layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gnPwyJPk53JH"
      },
      "outputs": [],
      "source": [
        "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02)\n",
        "\n",
        "class Clip(tf.keras.constraints.Constraint):\n",
        "    def __call__(self, w):\n",
        "        return tf.math.minimum(0.01, tf.math.maximum(w, -0.01))\n",
        "\n",
        "constraint = Clip()\n",
        "\n",
        "def add_dense_layer_for_noise(\n",
        "    model,\n",
        "    input_dim, \n",
        "    output_dim,\n",
        "): \n",
        "    model.add(layers.Dense(\n",
        "        units=output_dim, \n",
        "        input_shape=(input_dim,), \n",
        "        use_bias=False\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization(\n",
        "        gamma_initializer=gamma_initializer\n",
        "    ))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "# image size will be doubled \n",
        "def add_conv2d_transpose(\n",
        "    model,\n",
        "    num_output_filters, \n",
        "    add_batch_norm=True\n",
        "):\n",
        "    model.add(layers.Conv2DTranspose(\n",
        "        num_output_filters, \n",
        "        5, # filter size\n",
        "        strides=2, \n",
        "        padding='same', \n",
        "        use_bias=False, \n",
        "        kernel_initializer=w_init,\n",
        "    ))\n",
        "    if add_batch_norm:\n",
        "      model.add(layers.BatchNormalization(\n",
        "          gamma_initializer=gamma_initializer\n",
        "      ))\n",
        "      model.add(layers.ReLU())\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d_for_input(model, input_dim, num_output_filters):\n",
        "    model.add(layers.Conv2D(\n",
        "      num_output_filters, \n",
        "      5, # filter size\n",
        "      strides=2, \n",
        "      padding='same',\n",
        "      input_shape=[input_dim, input_dim, 3],\n",
        "      kernel_initializer=w_init,\n",
        "      kernel_constraint=constraint,\n",
        "    ))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d(\n",
        "    model, num_output_filters, filter_size=5, \n",
        "    use_batch_norm=True, padding='same', stride=2\n",
        "):\n",
        "    model.add(layers.Conv2D(\n",
        "        num_output_filters, \n",
        "        filter_size, # filter size\n",
        "        strides=stride, \n",
        "        padding=padding,\n",
        "        kernel_initializer=w_init,\n",
        "        kernel_constraint=constraint,\n",
        "    ))\n",
        "    if use_batch_norm:\n",
        "      model.add(layers.BatchNormalization(\n",
        "          gamma_initializer=gamma_initializer, \n",
        "          beta_constraint=constraint, \n",
        "          gamma_constraint=constraint\n",
        "      ))\n",
        "      model.add(layers.LeakyReLU(0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "po5f1FHj7Zl1"
      },
      "outputs": [],
      "source": [
        "def create_unconditional_generator(\n",
        "    noise_dim,\n",
        "    image_dim, # output image\n",
        "):\n",
        "    model = tf.keras.Sequential()\n",
        "    add_dense_layer_for_noise(\n",
        "        model, input_dim=noise_dim, \n",
        "        output_dim=(image_dim * 8) * (image_dim/16) * (image_dim/16)\n",
        "    )\n",
        "\n",
        "    model.add(layers.Reshape(\n",
        "        (int(image_dim/16), int(image_dim/16), image_dim * 8))\n",
        "    ) # image_dim/16 * image_dim/16 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, image_dim * 4) # image_dim/8 * image_dim/8 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 2) # image_dim/4 * image_dim/4 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 1) # image_dim/2 * image_dim/2 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, 3, add_batch_norm=False) # image_dim * image_dim * 3\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "def create_discriminator(image_dim): \n",
        "    model = tf.keras.Sequential()\n",
        "    add_conv2d_for_input(model, image_dim, image_dim) # (image_dim /2, image_dim /2, image_dim)\n",
        "\n",
        "    add_conv2d(model, image_dim * 2) # (image_dim /4, image_dim /4, image_dim * 2)\n",
        "    add_conv2d(model, image_dim * 4) # (image_dim /8, image_dim /8, image_dim * 4)\n",
        "    add_conv2d(model, image_dim * 8) # (image_dim /16, image_dim /16, image_dim * 8)\n",
        "\n",
        "    add_conv2d(model, 1, filter_size=int(image_dim/16), use_batch_norm=False, padding='valid', stride=1) # (1, 1, 1)\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MofdVvWU-RxF"
      },
      "outputs": [],
      "source": [
        "def validate_generator():\n",
        "  # TODO: validate layers dimensions\n",
        "  generator = create_unconditional_generator(100, 64)\n",
        "\n",
        "\n",
        "def validate_discriminator():\n",
        "  # TODO\n",
        "  discriminator = create_discriminator(64)\n",
        "\n",
        "def test_output_values():\n",
        "    generator = create_unconditional_generator(100, 64)\n",
        "    discriminator = create_discriminator(64)\n",
        "    noise = tf.random.normal([10, 100])\n",
        "    fake_images = generator(noise, training=True)\n",
        "    output = discriminator(fake_images, training=True)\n",
        "    print(tf.reduce_mean(output))\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hazX8bT1-sEE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions \n",
        "def discriminator_loss_wasserstein(real_output, fake_output): \n",
        "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "def generator_loss_wasserstein(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)"
      ],
      "metadata": {
        "id": "MfFdV1yZd4nL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xrKWpvuoX9Fr"
      },
      "outputs": [],
      "source": [
        "# training step \n",
        "\n",
        "# may use @tf.function for optimization, but have to deal with dynamic variable step\n",
        "def train_step(\n",
        "    image_batch, batch_size, noise_dim, generator, discriminator, \n",
        "    generator_optimizer, discriminator_optimizer, discriminator_loss_func, \n",
        "    generator_loss_func, step\n",
        "):\n",
        "    # TODO: validate that image_batch size is same as batch_size\n",
        "    # TODO：fine tune ratio of frequency that generator and discriminator are trained\n",
        "    metrics = {}\n",
        "    # 1. update discriminator \n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(image_batch, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        disc_loss = discriminator_loss_func(real_output, fake_output)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    metrics['D_loss'] = disc_loss\n",
        "  \n",
        "    # 2.update generator\n",
        "    if (step + 1) % 5 == 0:\n",
        "        noise = tf.random.normal([batch_size, noise_dim])\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            generated_images = generator(noise, training=True)\n",
        "            fake_output = discriminator(generated_images, training=True)\n",
        "            gen_loss = generator_loss_func(fake_output)\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "        metrics['G_loss'] = gen_loss\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_plot(examples, epoch, n):\n",
        "    examples = (examples + 1) / 2.0\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(examples[i])  \n",
        "    filename = f\"samples/generated_plot_epoch-{epoch+1}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MGdoTgKvDVHu"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "IMAGE_DIM = 64\n",
        "NOISE_DIM = 100\n",
        "TEST_IMAGE_GRID_SIZE = 4\n",
        "\n",
        "CLIP_VALUE_FOR_WGAN = 0.01\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCH = 50\n",
        "\n",
        "def train(save_model=True):\n",
        "    test_noises = tf.random.normal([TEST_IMAGE_GRID_SIZE ** 2, NOISE_DIM])\n",
        "    image_batches = load_dataset(WORKSPACE_DIR, BATCH_SIZE, (IMAGE_DIM, IMAGE_DIM))\n",
        "\n",
        "    generator = create_unconditional_generator(NOISE_DIM, IMAGE_DIM)\n",
        "    generator.summary()\n",
        "    discriminator = create_discriminator(IMAGE_DIM)\n",
        "    discriminator.summary()\n",
        "    generator_loss_func = generator_loss_wasserstein\n",
        "    discriminator_loss_func = discriminator_loss_wasserstein\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(NUM_EPOCH):\n",
        "        pbar = tf.keras.utils.Progbar(target=int(image_batches.cardinality()), stateful_metrics=[])\n",
        "        i = 0 \n",
        "        metrics = {'epoch': epoch}\n",
        "        for image_batch in image_batches:\n",
        "            metrics.update(train_step(\n",
        "                image_batch=image_batch, \n",
        "                batch_size=BATCH_SIZE, \n",
        "                noise_dim=NOISE_DIM, \n",
        "                generator=generator, \n",
        "                discriminator=discriminator, \n",
        "                generator_optimizer=generator_optimizer, \n",
        "                discriminator_optimizer=discriminator_optimizer, \n",
        "                discriminator_loss_func=discriminator_loss_func, \n",
        "                generator_loss_func=generator_loss_func,\n",
        "                step=step,\n",
        "            ))\n",
        "            pbar.update(i, values=metrics.items(), finalize=False)\n",
        "            i += 1\n",
        "            step += 1\n",
        "\n",
        "        pbar.update(step, values=metrics.items(), finalize=True)\n",
        "        save_plot(generator(test_noises, training=False), epoch, TEST_IMAGE_GRID_SIZE)\n",
        "        if save_model:\n",
        "          generator.save(f\"saved_model/generator_epoch-{epoch}.h5\")\n",
        "          discriminator.save(f\"saved_model/discriminator_epoch-{epoch}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download models\n",
        "from google.colab import files\n",
        "files.download('saved_model/generator_epoch-50.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UFSsbW04mQyO",
        "outputId": "5f51f84a-f547-48cc-9ace-20de013dc175"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_38573502-85b2-4597-b133-926a443087bd\", \"generator_W_epoch-50.h5\", 20685408)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "JdVxDF8sOPuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNrTRXiN+6hX4r3TVe56+O4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}