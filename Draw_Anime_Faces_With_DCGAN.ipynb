{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/FunModels/blob/main/Draw_Anime_Faces_With_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK_I7KqkoZVj"
      },
      "source": [
        "# Draw Anime Faces With Generative Adversarial Network\n",
        "\n",
        "- The model uses DCGAN architecture per https://arxiv.org/abs/1511.06434\n",
        "- Tensorflow is used as the training framework \n",
        "- The code isn't very super robust as validations are left to be implemented "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I08L7GiojrP"
      },
      "source": [
        "## Set up the environment \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S14DtgS1o1oX"
      },
      "outputs": [],
      "source": [
        "WORKSPACE_DIR = '.' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxbrMAVxovUU"
      },
      "source": [
        "### Download the dataset \n",
        "The dataset is collected by https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdEVoLFoYIH",
        "outputId": "5745ead8-09ba-4ca0-cd40-c2313da032b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p\n",
            "To: /content/crypko_data.zip\n",
            "100% 452M/452M [00:03<00:00, 116MB/s]\n"
          ]
        }
      ],
      "source": [
        "# a pypi package to download large file from google drive \n",
        "!gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p -O \"{WORKSPACE_DIR}/crypko_data.zip\"\n",
        "!unzip -q -o \"{WORKSPACE_DIR}/crypko_data.zip\" -d \"{WORKSPACE_DIR}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfwsQHztcPP"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6VBt4P8YLQnZ"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auocQWduLJc"
      },
      "source": [
        "## Data Preprocessing \n",
        "1. Load Dataset From Directory\n",
        "2. Resize the imaqe\n",
        "3. **Normalize Image: it's very very very important that the image data is withint [-1, 1] for neural network!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIam_D6Mz48I"
      },
      "outputs": [],
      "source": [
        "def load_dataset(directory_path, batch_size, image_size): \n",
        "    images = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory_path, \n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        image_size=image_size\n",
        "    )\n",
        "    normalization_layer = tf.keras.layers.Rescaling(2.0/255, offset=-1)\n",
        "    return images.map(lambda x: normalization_layer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nCCNafSI4s5k"
      },
      "outputs": [],
      "source": [
        "def validate_data_loading(): \n",
        "  image_batches = load_dataset(WORKSPACE_DIR, 64, (64, 64))\n",
        "  # TODO: \n",
        "  # 1.validate dimension is (batch_size, height, width, 3)\n",
        "  # 2.validate that all values are within [-1, 1]\n",
        "  # 3.display 16 images \n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  data = image_batches[0].take(16).map(lambda image: tf.keras.layers.Rescaling(255/2.0, offset=127.5)(image))\n",
        "  # TODO: better way of display tensors \n",
        "  for i, image in enumerate(data):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nthVTZ7zHfgx"
      },
      "source": [
        "## Define Model Architecture \n",
        "Two Models are defined with Keras layers, aka Generator model and Discriminator model. \n",
        "\n",
        "DCGAN key points: \n",
        "- Generator consists of convolutional -transpose layers that given a latent vector of smaller dimension, generates a 2D image with larger dimension\n",
        "- discriminator consists of convolutional layers, takes the large dimension 2D image, convolutes and eventually generate a binary output\n",
        "- apply batch normalization after each layer, except for output layer for generator and input layer for discriminator. \n",
        "- apply random normal distribution for weight initialization convolution(transpose) layers \n",
        "- apply ReLU activation for convolution transpose layers and leaky ReLU for convolution layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gnPwyJPk53JH"
      },
      "outputs": [],
      "source": [
        "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02)\n",
        "\n",
        "def add_dense_layer_for_noise(\n",
        "    model,\n",
        "    input_dim, \n",
        "    output_dim,\n",
        "): \n",
        "    model.add(layers.Dense(units=output_dim, input_shape=(input_dim,), use_bias=False,))\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=gamma_initializer))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "# image size will be doubled \n",
        "def add_conv2d_transpose(\n",
        "    model,\n",
        "    num_output_filters, \n",
        "    add_batch_norm=True\n",
        "):\n",
        "    model.add(layers.Conv2DTranspose(\n",
        "        num_output_filters, \n",
        "        5, # filter size\n",
        "        strides=2, \n",
        "        padding='same', \n",
        "        use_bias=False, \n",
        "        kernel_initializer=w_init,\n",
        "    ))\n",
        "    if add_batch_norm:\n",
        "      model.add(layers.BatchNormalization(gamma_initializer=gamma_initializer))\n",
        "      model.add(layers.ReLU())\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d_for_input(model, input_dim, num_output_filters):\n",
        "    model.add(layers.Conv2D(\n",
        "      num_output_filters, \n",
        "      5, # filter size\n",
        "      strides=2, \n",
        "      padding='same',\n",
        "      input_shape=[input_dim, input_dim, 3],\n",
        "      kernel_initializer=w_init\n",
        "    ))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "# shrink size by half\n",
        "def add_conv2d(model, num_output_filters, filter_size=5, use_batch_norm=True, padding='same', stride=2):\n",
        "    model.add(layers.Conv2D(\n",
        "        num_output_filters, \n",
        "        filter_size, # filter size\n",
        "        strides=stride, \n",
        "        padding=padding,\n",
        "        kernel_initializer=w_init,\n",
        "    ))\n",
        "    if use_batch_norm:\n",
        "      model.add(layers.BatchNormalization(gamma_initializer=gamma_initializer))\n",
        "      model.add(layers.LeakyReLU(0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "po5f1FHj7Zl1"
      },
      "outputs": [],
      "source": [
        "def create_unconditional_generator(\n",
        "    noise_dim,\n",
        "    image_dim, # output image\n",
        "):\n",
        "    model = tf.keras.Sequential()\n",
        "    add_dense_layer_for_noise(model, input_dim=noise_dim, output_dim=(image_dim * 8) * (image_dim/16) * (image_dim/16))\n",
        "\n",
        "    model.add(layers.Reshape((int(image_dim/16), int(image_dim/16), image_dim * 8))) # image_dim/16 * image_dim/16 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, image_dim * 4) # image_dim/8 * image_dim/8 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 2) # image_dim/4 * image_dim/4 * filters\n",
        "    add_conv2d_transpose(model, image_dim * 1) # image_dim/2 * image_dim/2 * filters\n",
        "\n",
        "    add_conv2d_transpose(model, 3, add_batch_norm=False) # image_dim * image_dim * 3\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "def create_discriminator(image_dim): \n",
        "    model = tf.keras.Sequential()\n",
        "    add_conv2d_for_input(model, image_dim, image_dim) # (image_dim /2, image_dim /2, image_dim)\n",
        "\n",
        "    add_conv2d(model, image_dim * 2) # (image_dim /4, image_dim /4, image_dim * 2)\n",
        "    add_conv2d(model, image_dim * 4) # (image_dim /8, image_dim /8, image_dim * 4)\n",
        "    add_conv2d(model, image_dim * 8) # (image_dim /16, image_dim /16, image_dim * 8)\n",
        "\n",
        "    add_conv2d(model, 1, filter_size=int(image_dim/16), use_batch_norm=False, padding='valid', stride=1) # (1, 1, 1)\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MofdVvWU-RxF"
      },
      "outputs": [],
      "source": [
        "def validate_generator():\n",
        "  # TODO: validate layers dimensions\n",
        "  generator = create_unconditional_generator(100, 64)\n",
        "\n",
        "def validate_discriminator():\n",
        "  # TODO\n",
        "  discriminator = create_discriminator(64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hazX8bT1-sEE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MfFdV1yZd4nL"
      },
      "outputs": [],
      "source": [
        "# loss functions \n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "def discriminator_loss_entropy(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss_entropy(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xrKWpvuoX9Fr"
      },
      "outputs": [],
      "source": [
        "# training step \n",
        "\n",
        "# may use @tf.function for optimization, but have to deal with dynamic variable step\n",
        "def train_step(image_batch, batch_size, noise_dim, generator, discriminator, generator_optimizer, discriminator_optimizer, discriminator_loss_func, generator_loss_func, step):\n",
        "    # TODO: validate that image_batch size is same as batch_size\n",
        "    # TODOï¼šfine tune ratio of frequency that generator and discriminator are trained\n",
        "    metrics = {}\n",
        "    # 1. update discriminator \n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(image_batch, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        disc_loss = discriminator_loss_func(real_output, fake_output)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    metrics['D_loss'] = disc_loss\n",
        "  \n",
        "    # 2.update generator\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        gen_loss = generator_loss_func(fake_output)\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    metrics['G_loss'] = gen_loss\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_plot(examples, epoch, n):\n",
        "    examples = (examples + 1) / 2.0\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(examples[i])  \n",
        "    filename = f\"samples/generated_plot_epoch-{epoch+1}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MGdoTgKvDVHu"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "IMAGE_DIM = 64\n",
        "NOISE_DIM = 100\n",
        "TEST_IMAGE_GRID_SIZE = 4\n",
        "\n",
        "CLIP_VALUE_FOR_WGAN = 0.01\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCH = 50\n",
        "\n",
        "def train(loss_function=None, save_model=True):\n",
        "    test_noises = tf.random.normal([TEST_IMAGE_GRID_SIZE ** 2, NOISE_DIM])\n",
        "    image_batches = load_dataset(WORKSPACE_DIR, BATCH_SIZE, (IMAGE_DIM, IMAGE_DIM))\n",
        "\n",
        "    generator = create_unconditional_generator(NOISE_DIM, IMAGE_DIM)\n",
        "    generator.summary()\n",
        "    discriminator = create_discriminator(IMAGE_DIM)\n",
        "    discriminator.summary()\n",
        "    generator_loss_func = generator_loss_entropy\n",
        "    discriminator_loss_func = discriminator_loss_entropy\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
        "    \n",
        "    step = 0\n",
        "    for epoch in range(NUM_EPOCH):\n",
        "        pbar = tf.keras.utils.Progbar(target=int(image_batches.cardinality()), stateful_metrics=[])\n",
        "        i = 0\n",
        "        metrics = {'epoch': epoch}\n",
        "        for image_batch in image_batches:\n",
        "            metrics.update(train_step(\n",
        "                image_batch=image_batch, \n",
        "                batch_size=BATCH_SIZE, \n",
        "                noise_dim=NOISE_DIM, \n",
        "                generator=generator, \n",
        "                discriminator=discriminator, \n",
        "                generator_optimizer=generator_optimizer, \n",
        "                discriminator_optimizer=discriminator_optimizer, \n",
        "                discriminator_loss_func=discriminator_loss_func, \n",
        "                generator_loss_func=generator_loss_func,\n",
        "                step=step,\n",
        "            ))\n",
        "            pbar.update(i, values=metrics.items(), finalize=False)\n",
        "            i += 1\n",
        "            step += 1\n",
        "\n",
        "        pbar.update(step, values=metrics.items(), finalize=True)\n",
        "        save_plot(generator(test_noises, training=False), epoch, TEST_IMAGE_GRID_SIZE)\n",
        "        if save_model:\n",
        "          generator.save(f\"saved_model/generator_{loss_function}_epoch-{epoch}.h5\")\n",
        "          discriminator.save(f\"saved_model/discriminator_{loss_function}_epoch-{epoch}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdVxDF8sOPuQ"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNMMXZ3VZSLzCUPxslunfTM",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}